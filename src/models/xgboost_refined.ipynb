{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99da0483-246f-4288-856c-1b4f2bd8486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /opt/anaconda3/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from lightgbm) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3760c66-ad59-4dd8-b0a6-e5ad3e0c3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from lightgbm import early_stopping, log_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c51916-bf97-475a-b101-fa700bcd33b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 1) Embedding Model\n",
    "##############################################\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', device=None):\n",
    "        \"\"\"\n",
    "        Load a SentenceTransformer model on GPU \n",
    "        if available (or CPU otherwise).\n",
    "        \"\"\"\n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = SentenceTransformer(model_name, device=self.device)\n",
    "\n",
    "    def encode(self, texts, show_progress_bar=False):\n",
    "        \"\"\"\n",
    "        Encode one string or a list of strings into a (n×d) numpy.float32 array.\n",
    "        \"\"\"\n",
    "        # Normalize input to list\n",
    "        inputs = [texts] if isinstance(texts, str) else texts\n",
    "        embeddings = self.model.encode(inputs, show_progress_bar=show_progress_bar and len(inputs) > 1)\n",
    "        return np.array(embeddings, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c050b5b-a8fd-4751-b3d7-acccfe4d91ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSVs...\n",
      "Train shape: (7921, 18)\n",
      "Val shape:   (987, 18)\n",
      "Test shape:  (987, 18)\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# 2) Load Data in Wide Format\n",
    "##############################################\n",
    "# Each CSV has rows = topics, columns = [2007..2025].\n",
    "# We'll label them as train, val, test by topics (not year).\n",
    "##############################################\n",
    "print(\"Loading CSVs...\")\n",
    "train_wide = pd.read_csv(\"train_data_new.csv\", index_col=0)\n",
    "val_wide   = pd.read_csv(\"val_data_new.csv\", index_col=0)\n",
    "test_wide  = pd.read_csv(\"test_data_new.csv\", index_col=0)\n",
    "\n",
    "# Drop 2025 column if it exists\n",
    "for df in [train_wide, val_wide, test_wide]:\n",
    "    if \"2025\" in df.columns:\n",
    "        df.drop(columns=[\"2025\"], inplace=True)\n",
    "        \n",
    "print(\"Train shape:\", train_wide.shape)\n",
    "print(\"Val shape:  \", val_wide.shape)\n",
    "print(\"Test shape: \", test_wide.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ebb6b33-58c8-415d-b068-59d687ac1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 3) Sliding Window Function\n",
    "##############################################\n",
    "def generate_sliding_samples(wide_df, input_len, output_len):\n",
    "    \"\"\"\n",
    "    wide_df: DataFrame with row=topic, cols= [2007..2025], numeric counts.\n",
    "    input_len: how many years to use as input\n",
    "    output_len: how many future years to predict\n",
    "    Returns: a new DataFrame, each row is a sliding window sample with:\n",
    "      f0..f(input_len-1) = the input counts\n",
    "      y1..y(output_len)  = the future counts\n",
    "      'topic': the topic name\n",
    "      'last_input_year': the year of the final input\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    topics = wide_df.index\n",
    "    \n",
    "    # Convert the columns to strings to iterate easily\n",
    "    year_cols = [str(c) for c in wide_df.columns]\n",
    "    \n",
    "    for topic in topics:\n",
    "        row_values = wide_df.loc[topic].values.astype(float)  # all year counts\n",
    "        n_years = len(row_values)\n",
    "        \n",
    "        # We'll assume columns are in ascending year order. If not, sort them first.\n",
    "        # For i in range( n_years - (input_len+output_len) + 1 ):\n",
    "        for i in range(n_years - input_len - output_len + 1):\n",
    "            # Input slice\n",
    "            x_slice = row_values[i : i + input_len]\n",
    "            # Target slice\n",
    "            y_slice = row_values[i + input_len : i + input_len + output_len]\n",
    "            \n",
    "            # The last input year's index\n",
    "            # year index = i + input_len - 1 in zero-based offset\n",
    "            final_input_index = (i + input_len - 1)\n",
    "            \n",
    "            # We'll store the actual numeric year from the columns\n",
    "            last_input_year_str = year_cols[final_input_index]\n",
    "            last_input_year = int(last_input_year_str)\n",
    "            \n",
    "            sample_dict = {}\n",
    "            # Fill input\n",
    "            for idx, val in enumerate(x_slice):\n",
    "                sample_dict[f\"f{idx}\"] = val\n",
    "            # Fill target\n",
    "            for idx, val in enumerate(y_slice):\n",
    "                sample_dict[f\"y{idx+1}\"] = val\n",
    "            \n",
    "            sample_dict[\"topic\"] = topic\n",
    "            sample_dict[\"last_input_year\"] = last_input_year\n",
    "            \n",
    "            samples.append(sample_dict)\n",
    "    \n",
    "    out_df = pd.DataFrame(samples)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a5de9ac-fd15-4cdb-ba09-ff9345f78b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape: (86460, 8)\n",
      "val_df shape:   (985, 8)\n",
      "test_df shape:  (987, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##############################################\n",
    "# 4) Generate sliding samples for each subset\n",
    "##############################################\n",
    "input_len  = 5\n",
    "output_len = 1\n",
    "\n",
    "train_df = generate_sliding_samples(train_wide, input_len, output_len)\n",
    "train_df = train_df[train_df[\"last_input_year\"] <= 2021]\n",
    "val_df   = generate_sliding_samples(val_wide,   input_len, output_len)\n",
    "val_df = val_df[val_df[\"last_input_year\"] == 2022]\n",
    "test_df  = generate_sliding_samples(test_wide,  input_len, output_len)\n",
    "test_df = test_df[test_df[\"last_input_year\"] == 2023]\n",
    "\n",
    "print(\"train_df shape:\", train_df.shape)\n",
    "print(\"val_df shape:  \", val_df.shape)\n",
    "print(\"test_df shape: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20e2f0d5-a4b2-4c34-88aa-4ee034b55d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 5) Add year as a numeric feature, plus stats\n",
    "##############################################\n",
    "def add_basic_features(df, input_len):\n",
    "    fcols = [f\"f{i}\" for i in range(input_len)]\n",
    "    \n",
    "    # Summary stats\n",
    "    df[\"f_mean\"] = df[fcols].mean(axis=1)\n",
    "    df[\"f_std\"]  = df[fcols].std(axis=1)\n",
    "    df[\"f_min\"] = df[fcols].min(axis=1)\n",
    "    df[\"f_max\"] = df[fcols].max(axis=1)\n",
    "    df[\"f_last_minus_first\"] = df[fcols[-1]] - df[fcols[0]]\n",
    "    df[\"linear_trend\"] = df[fcols].apply(lambda row: np.polyfit(range(len(row)), row, 1)[0], axis=1)\n",
    "    df[\"slope_mean_ratio\"] = df[\"linear_trend\"] / (df[\"f_mean\"] + 1e-8)\n",
    "    \n",
    "    # Year features\n",
    "    df[\"norm_year\"] = df[\"last_input_year\"] - 2007\n",
    "    df[\"norm_year_sq\"] = df[\"norm_year\"] ** 2\n",
    "\n",
    "    # Deltas and recent patterns\n",
    "    df[\"f_diff_1\"] = df[\"f4\"] - df[\"f3\"]\n",
    "    df[\"f_diff_2\"] = df[\"f3\"] - df[\"f2\"]\n",
    "    df[\"f_diff_3\"] = df[\"f2\"] - df[\"f1\"]\n",
    "    df[\"f_diff_4\"] = df[\"f1\"] - df[\"f0\"]\n",
    "    df[\"f_acceleration\"] = (df[\"f4\"] - df[\"f3\"]) - (df[\"f3\"] - df[\"f2\"])\n",
    "    df[\"f_rolling_mean_3\"] = df[[\"f2\", \"f3\", \"f4\"]].mean(axis=1)\n",
    "\n",
    "    # Behavioral indicators\n",
    "    df[\"f_last_to_mean_ratio\"] = df[\"f4\"] / (df[\"f_mean\"] + 1e-8)\n",
    "    df[\"last_year_jump_ratio\"] = df[\"f4\"] / (df[\"f3\"] + 1e-8)\n",
    "    df[\"is_flat\"] = ((df[\"f4\"] - df[\"f0\"]).abs() < 200).astype(int)\n",
    "\n",
    "    df[\"recent_growth_strength\"] = df[\"f_diff_1\"] * df[\"last_year_jump_ratio\"]\n",
    "    \n",
    "    df[\"topic_size_bin\"] = pd.cut(\n",
    "        df[\"f_mean\"], \n",
    "        bins=[-1, 1000, 3000, 7000, float(\"inf\")], \n",
    "        labels=[0, 1, 2, 3]\n",
    "    ).astype(int)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = add_basic_features(train_df, input_len)\n",
    "val_df   = add_basic_features(val_df,   input_len)\n",
    "test_df  = add_basic_features(test_df,  input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c17be37-0673-4f61-87c2-40ee2491d530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding all topics (count: 9832 ) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d495ad7c724397b3a4c862bd634928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##############################################\n",
    "# 6) Topic Embeddings\n",
    "##############################################\n",
    "embedding_model = EmbeddingModel(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "all_topics = pd.concat([train_df[\"topic\"], val_df[\"topic\"], test_df[\"topic\"]]).unique()\n",
    "print(\"Embedding all topics (count:\", len(all_topics), \") ...\")\n",
    "\n",
    "embeddings = embedding_model.encode(all_topics, show_progress_bar=True)\n",
    "\n",
    "# Make a map: topic -> embedding\n",
    "topic_to_emb = dict(zip(all_topics, embeddings))\n",
    "\n",
    "def attach_embeddings(df):\n",
    "    emb_list = [topic_to_emb[t] for t in df[\"topic\"]]\n",
    "    emb_cols = [f\"emb_{i}\" for i in range(embeddings.shape[1])]\n",
    "    emb_df = pd.DataFrame(emb_list, columns=emb_cols, index=df.index)\n",
    "    return pd.concat([df, emb_df], axis=1)\n",
    "\n",
    "train_df = attach_embeddings(train_df)\n",
    "val_df   = attach_embeddings(val_df)\n",
    "test_df  = attach_embeddings(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c2fd00d-db9c-4b1d-b1cb-5c0e5e728e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 7) Prepare X, y for T+1\n",
    "##############################################\n",
    "# We'll train separate models for y1\n",
    "\n",
    "# Fixed list of all non-embedding features\n",
    "base_features = [\n",
    "    \"f0\", \"f1\", \"f2\", \"f3\", \"f4\",\n",
    "    \"f_mean\", \"f_std\", \"f_min\", \"f_max\",\n",
    "    \"f_last_minus_first\", \"linear_trend\",\"slope_mean_ratio\",\n",
    "    \"norm_year\", \"norm_year_sq\",\n",
    "    \"f_diff_1\", \"f_diff_2\", \"f_diff_3\", \"f_diff_4\",\n",
    "    \"f_acceleration\", \"f_rolling_mean_3\", \n",
    "    \"f_last_to_mean_ratio\", \"last_year_jump_ratio\", \"is_flat\",\n",
    "    \"recent_growth_strength\"\n",
    "]\n",
    "\n",
    "# Add topic embedding dimensions\n",
    "embedding_features = [col for col in train_df.columns if col.startswith(\"emb_\")]\n",
    "input_features = base_features + embedding_features\n",
    "input_features += [\"topic_size_bin\"]\n",
    "\n",
    "def train_and_eval_lgbm(target_col, label):\n",
    "    X_train = train_df[input_features]\n",
    "    X_val   = val_df[input_features]\n",
    "    X_test  = test_df[input_features]\n",
    "\n",
    "    y_train = train_df[target_col]\n",
    "    y_val   = val_df[target_col]\n",
    "    y_test  = test_df[target_col]\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        objective='regression',\n",
    "        learning_rate=0.05,\n",
    "        max_depth=12,\n",
    "        n_estimators=1000,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=1.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=1  # Optional: 0=silent, 1=info, 2=warning\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[\n",
    "            early_stopping(100),\n",
    "            log_evaluation(50)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "\n",
    "    print(f\"{label} RMSE: {rmse:.2f}\")\n",
    "    print(f\"{label} MAE:  {mae:.2f}\")\n",
    "\n",
    "    return model, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8723a7c1-4696-4f9e-af5b-79abed7a6c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 103305\n",
      "[LightGBM] [Info] Number of data points in the train set: 86460, number of used features: 409\n",
      "[LightGBM] [Info] Start training from score 2256.343141\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's rmse: 1636.96\tvalid_0's l2: 2.67965e+06\n",
      "[100]\tvalid_0's rmse: 1454.93\tvalid_0's l2: 2.11682e+06\n",
      "[150]\tvalid_0's rmse: 1440.78\tvalid_0's l2: 2.07586e+06\n",
      "[200]\tvalid_0's rmse: 1440.23\tvalid_0's l2: 2.07427e+06\n",
      "[250]\tvalid_0's rmse: 1441.99\tvalid_0's l2: 2.07933e+06\n",
      "Early stopping, best iteration is:\n",
      "[159]\tvalid_0's rmse: 1437.57\tvalid_0's l2: 2.06661e+06\n",
      "T+1 RMSE: 3306.25\n",
      "T+1 MAE:  2284.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train model for T+1 (y1)\n",
    "model_lgbm, pred_lgbm = train_and_eval_lgbm(\"y1\", \"T+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "eec7d9e8-d4bf-4430-9215-ff7e0783efeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All 3 CSVs saved.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Generate predictions for train and val as well\n",
    "pred_val_y1   = model_lgbm.predict(val_df[input_features]).round().astype(int)\n",
    "pred_test_y1  = pred_lgbm.round().astype(int)  # you already computed pred_lgbm\n",
    "\n",
    "# 1. Train: predict 2022\n",
    "# ✅ 1. Filter train samples that predict 2022\n",
    "train_2022 = train_df[train_df[\"last_input_year\"] == 2021].copy()\n",
    "\n",
    "# ✅ 2. Predict and round to int\n",
    "pred_train_y1 = model_lgbm.predict(train_2022[input_features]).round().astype(int)\n",
    "\n",
    "# ✅ 3. Export\n",
    "train_out = pd.DataFrame({\n",
    "    \"topic\": train_2022[\"topic\"].values,\n",
    "    \"actual_2022\": train_2022[\"y1\"].astype(int),\n",
    "    \"predicted_2022\": pred_train_y1\n",
    "})\n",
    "train_out.to_csv(\"train_predictions_2022.csv\", index=False)\n",
    "\n",
    "\n",
    "# 2. Validation: predict 2023\n",
    "val_out = pd.DataFrame({\n",
    "    \"topic\": val_df[\"topic\"].values,\n",
    "    \"actual_2023\": val_df[\"y1\"].values,\n",
    "    \"predicted_2023\": pred_val_y1\n",
    "})\n",
    "val_out.to_csv(\"val_predictions_2023.csv\", index=False)\n",
    "\n",
    "# 3. Test: predict 2024\n",
    "test_out = pd.DataFrame({\n",
    "    \"topic\": test_df[\"topic\"].values,\n",
    "    \"actual_2024\": test_df[\"y1\"].values,\n",
    "    \"predicted_2024\": pred_test_y1\n",
    "})\n",
    "test_out.to_csv(\"test_predictions_2024.csv\", index=False)\n",
    "\n",
    "print(\"✅ All 3 CSVs saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d38a4971-a3c3-4274-a12c-0225a3a253c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T+1 final test RMSE: 3850.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# 8) Summaries or Save\n",
    "##############################################\n",
    "import numpy as np\n",
    "\n",
    "# Combine final predictions\n",
    "test_preds = pd.DataFrame({\n",
    "    \"topic\": test_df[\"topic\"],\n",
    "    \"last_input_year\": test_df[\"last_input_year\"],\n",
    "    \"y1_true\": test_df[\"y1\"],\n",
    "    \"y1_pred\": pred_y1\n",
    "})\n",
    "\n",
    "rmse_i = mean_squared_error(test_preds[\"y1_true\"], test_preds[\"y1_pred\"], squared=False)\n",
    "print(f\"T+1 final test RMSE: {rmse_i:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c138c038-8ea4-410d-8e7b-9015f3428508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
